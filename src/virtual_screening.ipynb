{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import cuda\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchkge.sampling import BernoulliNegativeSampler\n",
    "from torchkge.utils import MarginLoss,DataLoader\n",
    "from torchkge import KnowledgeGraph,DistMultModel,TransEModel,TransRModel\n",
    "from torchkge.models.bilinear import HolEModel,ComplExModel\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem \n",
    "from rdkit.Chem import Lipinski\n",
    "from rdkit.Chem import Descriptors\n",
    "import tqdm\n",
    "\n",
    "def rule_five_calculator(df):\n",
    "    # INDEX = []\n",
    "    num_H_acc =[]\n",
    "    num_H_don = []\n",
    "    num_rota = []\n",
    "    mw = []\n",
    "    logp = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(df))):\n",
    "        smi = df.iloc[i]['smi']\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "\n",
    "        num_H_acc.append(Lipinski.NumHAcceptors(mol))\n",
    "        num_H_don.append(Lipinski.NumHDonors(mol))\n",
    "        num_rota.append(Lipinski.NumRotatableBonds(mol))\n",
    "        mw.append(Descriptors.MolWt(mol))\n",
    "        logp.append(Descriptors.MolLogP(mol))\n",
    "        # if num_H_acc < 10 and num_H_don <5 and mw <=500 and num_rota < 10 and logp>=1 and logp <=5:\n",
    "        #     INDEX.append(i)\n",
    "        #     num += 1\n",
    "\n",
    "    df['num_H_acc'] = num_H_acc\n",
    "    df['num_H_don'] = num_H_don\n",
    "    df['num_rota'] = num_rota\n",
    "    df['mw'] = mw\n",
    "    df['logp'] = logp\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import FilterCatalog\n",
    "# Initialize filter parameters\n",
    "param = FilterCatalog.FilterCatalogParams()\n",
    "# Add PAINS filters\n",
    "param.AddCatalog(FilterCatalog.FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "# Create the filter catalog\n",
    "filt = FilterCatalog.FilterCatalog(param)\n",
    "\n",
    "def PAINS_calculator(df):\n",
    "    pains = []\n",
    "    for i in tqdm.tqdm(range(len(df))):\n",
    "        smi = df.iloc[i]['smi']\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        pains.append(filt.HasMatch(mol))\n",
    "    \n",
    "    df['pains'] = pains\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This section is user defined !!!\n",
    "'''\n",
    "h_dim = 300\n",
    "\n",
    "data_path = \"../processed_data/target_inference_2/\"\n",
    "save_model_path = '../best_model/target_inference_2/'\n",
    "output_path = \"../results/target_inference_2/\"\n",
    "\n",
    "ent_list = ['Protein:ENPP1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load processed_data after training\n",
    "cause = pd.read_csv(data_path + 'cause.txt',sep='\\t',names=['from','rel','to'])\n",
    "ent2id = np.load(data_path + 'ent2id.npy', allow_pickle=True).item()\n",
    "rel2id = np.load(data_path + 'rel2id.npy', allow_pickle=True).item()\n",
    "\n",
    "h_cand = [v for k,v in ent2id.items() if k.startswith('CID:')]\n",
    "h_cand_ent = [k for k,v in ent2id.items() if k.startswith('CID:')]\n",
    "\n",
    "t_cand = [v for k,v in ent2id.items() if k.startswith(('Protein:','TF:','RBP:'))]\n",
    "t_cand_ent = [k for k,v in ent2id.items() if k.startswith(('Protein:','TF:','RBP:'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:07<00:00,  3.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# count virtual screening score\n",
    "vs_dict = {}\n",
    "\n",
    "for ent in tqdm.tqdm(ent_list):\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        model = DistMultModel(h_dim, len(ent2id), len(rel2id))\n",
    "        model.load_state_dict(torch.load(save_model_path + \"pertkg{}.pt\".format(i)))\n",
    "        if cuda.is_available():\n",
    "            cuda.empty_cache()\n",
    "            model.cuda()\n",
    "        model.normalize_parameters()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            ent_emb,rel_emb = model.get_embeddings() # (n_ent, emb_dim)\n",
    "            score = inference(ent,\n",
    "                            ent2id,rel2id,\n",
    "                            ent_emb,rel_emb,\n",
    "                            h_cand,t_cand,\n",
    "                            'virtual_screening')\n",
    "            results.append(score)\n",
    "\n",
    "    average_list = [sum(x) / len(x) for x in zip(*results)]\n",
    "    vs_dict['{}'.format(ent)] = average_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11062/11062 [00:10<00:00, 1060.70it/s]\n",
      "100%|██████████| 11062/11062 [00:35<00:00, 310.13it/s]\n",
      "100%|██████████| 11062/11062 [00:10<00:00, 1072.93it/s]\n",
      "100%|██████████| 11062/11062 [00:35<00:00, 311.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "comp_map_table = pd.read_csv('../map_file/compound_map_table.txt',sep='\\t')\n",
    "cid2smi = dict(zip(comp_map_table['cid'],comp_map_table['smi']))\n",
    "h_cand_smi = [cid2smi[x] for x in h_cand_ent]\n",
    "\n",
    "for k,v in vs_dict.items():\n",
    "    vs_score = v\n",
    "    df = pd.DataFrame({'compound':h_cand_ent,\n",
    "                       'vs_score':vs_score,\n",
    "                       'smi':h_cand_smi})\n",
    "    df = df.sort_values(by='vs_score', ascending=False)\n",
    "\n",
    "    # filter\n",
    "    df = rule_five_calculator(df)\n",
    "    df = PAINS_calculator(df)\n",
    "\n",
    "    df.to_csv(output_path + '{}.txt'.format(k),sep='\\t',index=False,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
